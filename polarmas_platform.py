import itertools
from datetime import datetime
from time import sleep
import pandas as pd
from google.api_core.exceptions import ResourceExhausted
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import SystemMessage, HumanMessage
import json
from dotenv import load_dotenv
import os

load_dotenv()
os.getenv("GOOGLE_API_KEY")

class Agent:
    """
    Represents a conversational agent with a defined persona, political stance, and demographic background.

    The agent can participate in conversations by generating responses based on its identity and message history,
    or it can passively observe conversations without responding. It uses a large language model (LLM) to
    generate context-aware responses and maintains a memory of messages per conversation thread.

    Attributes:
       llm (ChatGoogleGenerativeAI): The language model used for generating responses, set to "gemini-2.0-flash" with a temperature of 0.7.
       Requires the 'GOOGLE_API_KEY' environment variable to be set.
       name (str): A unique identifier for the agent.
       political_standpoint (str): The agent's political orientation or ideological stance.
       persona_description (str): A second-person narrative describing the agent’s personality traits, beliefs, and behaviors.
       demographics (str): A second-person narrative describing the agent’s demographic background.
       is_observer (bool): Indicates whether the agent is an active participant or a passive observer.
       memory (dict): A dictionary storing conversations keyed by conversation ID, with values being lists of message objects.

    Methods:
        respond(message, conversation_id):
            Generates a response to the given message using the agent’s attributes and conversation context.

        observe(message, conversation_id):
            Stores the message in memory without generating a response. Useful for passive observation.
    """
    def __init__(self, name, political_standpoint, persona_description, demographics, is_observer):
        """
        Initializes the Agent with identity attributes and sets up the LLM and memory store.

        Args:
            name (str): A unique identifier for the agent.
            political_standpoint (str): The agent's political orientation or ideological stance.
            persona_description (str): A second-person narrative string defining the agent's personality traits, beliefs, and behaviors.
            demographics (str): A second-person narrative string describing the agent's demographic background.
            is_observer (bool): Flag indicating whether the agent is an active participant or a passive observer in interactions.
        """
        self.llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.7)
        self.name = name
        self.political_standpoint = political_standpoint
        self.persona_description = persona_description
        self.demographics = demographics
        self.is_observer = is_observer
        self.memory = {}

    def respond(self, message, conversation_id):
        """
        Generates a response to an input message using the agent's predefined attributes and conversation memory.

        Args:
            message (Union[str, list[str]]): The incoming message/s to which the agent should respond.
            conversation_id (int): The identifier for the conversation thread. This is used to track and retrieve prior context.

        Returns:
            str: The content of the agent’s response, generated by the LLM.
        """
        self.memory[conversation_id] = [HumanMessage(content=message)]
        while True:
            try:
                response = self.llm(
                    [SystemMessage(content=f"You are {self.name}, {self.political_standpoint}.{self.demographics} {self.persona_description}"),
                     *sum(self.memory.values(), [])])
                break
            except ResourceExhausted as e:
                retry_delay = e.details[2].retry_delay.seconds
                print('Retrying in {} seconds...'.format(retry_delay))
                sleep(retry_delay)
        self.memory[conversation_id].append(response)
        return response.content

    def observe(self, message, conversation_id):
        """
        Records a message in the agent's memory without generating a response.

        Args:
            message (Union[str, list[str]]): The message/s to be observed and stored.
            conversation_id (id): The identifier for the conversation thread. Used to group messages by dialogue context.
        """
        self.memory[conversation_id] = [HumanMessage(content=message)]


class Platform:
    """
    A platform for simulating coordinated discussions among multiple LLM-based agents.

    This framework supports structured, turn-based conversations initiated by a context-rich prompt
    and guided by pre- and post-questionnaires that probe affective variables. Agents are defined
    by attributes such as political standpoint, persona, and demographics, and are either active
    participants or observers in the discussion.

    Attributes:
        agents (list): A list of Agent objects, initialized from a configuration CSV.
        pre_questionnaire (str): Prompt used to assess affective variables before the discussion.
        post_questionnaire (str): Prompt used to assess affective variables after the discussion.
        discussion_trigger (str): The combined topic and context prompt that initiates the conversation.
        memory_order (list): Ordered phases of the platform: ['pre_questionnaire', 'discussion', 'post_questionnaire'].
        logs (dict): Stores agent configuration filename, questionnaires responses and discussion messages.
    """

    def __init__(self, agents_config_path, pre_questionnaire, post_questionnaire, discussion_trigger):
        """
        Initializes the Platform instance with agents, questionnaires, and a discussion starter.

        Loads a set of agents from a CSV configuration file. Sets up pre- and post-discussion
        questionnaires, and formats the discussion prompt to initiate the conversation. Also prepares
        internal logging structures to record the simulation's flow and results.

        Args:
            agents_config_path (str): Path to a CSV file containing agent data. The CSV must include
                the columns: 'political_standpoint', 'persona_description', 'demographics', and 'is_observer'.
            pre_questionnaire (str): Prompt string used to assess baseline affective variables before discussion.
            post_questionnaire (str): Prompt string used to evaluate affective variables after the discussion.
            discussion_trigger (str): The initial prompt combining the topic, context, and instructions to guide agent interaction.
        """
        self.agents = self._initialize_agents(pd.read_csv(agents_config_path))
        self.pre_questionnaire = pre_questionnaire
        self.post_questionnaire = post_questionnaire
        self.discussion_trigger = discussion_trigger
        self.memory_order = ['pre_questionnaire', 'discussion', 'post_questionnaire']
        self.logs = self._initialize_logs(agents_config_path)
    def _initialize_agents(self, agents_df):
        return [
            Agent(
                f'Agent{idx}',
                config.political_standpoint,
                config.persona_description,
                config.demographics,
                config.is_observer
            )
            for idx, config in enumerate(agents_df.itertuples(index=False))
        ]
    def _initialize_logs(self, config_path):
        return {
            'agents_config': config_path,
            'pre_questionnaire': {},
            'post_questionnaire': {},
            'discussion': []
        }

    def facilitate_discussion(self, total_number_messages, verbose=True):
        """
        Orchestrates the full multi-agent conversation process.

        The conversation includes three main phases:
        1. Pre-questionnaire: Each agent responds to a baseline affective prompt.
        2. Discussion: Non-observer agents participate in a Round Robin exchange triggered by the prompt.
        3. Post-questionnaire: All agents respond to a follow-up prompt, reflecting on the conversation.

        All interactions are logged for later analysis or saving.

        Args:
            total_number_messages (int): Total number of messages to be exchanged in the discussion phase.
            verbose (bool): If True, prints progress and responses to stdout.
        """
        if verbose:
            self._print_section_header("Initial Questionnaire")
        for agent in self.agents:
            response = agent.respond(self.pre_questionnaire, self.memory_order.index('pre_questionnaire'))
            self.logs['pre_questionnaire'][agent.name] = response
            if verbose:
                self._print_agent_response(agent, response)

        if verbose:
            self._print_section_header("Discussion Begins")
        messages = [self.discussion_trigger]
        discussing_agents = itertools.cycle(agent for agent in self.agents if not agent.is_observer)
        while len(messages) <= total_number_messages:
            agent = next(discussing_agents)
            response = agent.respond(messages, self.memory_order.index('discussion'))
            messages.append(response)
            if verbose:
                print(response)

        if verbose:
            self._print_section_header("Post Discussion Questionnaire")
        for agent in self.agents:
            agent.observe(messages, self.memory_order.index('discussion'))
            self.logs['discussion'] = messages
            response = agent.respond(self.post_questionnaire, self.memory_order.index('post_questionnaire'))
            self.logs['post_questionnaire'][agent.name] = response
            if verbose:
                self._print_agent_response(agent, response)

    def _print_section_header(self, title):
        print(f"-- {title} --")

    def _print_agent_response(self, agent, response):
        print(f'- {agent.name} - {agent.political_standpoint} -')
        print(response)

    """
    Save the current run to a JSON file with a timestamped filename.

    The saved data includes the agents' configuration filename, the pre- and post-questionnaires, and the full discussion messages.

    Args:
        path (str): Directory path where the file should be saved.
        filename (str): Base name for the file (timestamp will be appended).

    The file will be saved as '<filename>_YYYY-MM-DD_HH-MM-SS.json'.
    If saving fails, an error message will be printed.
    """
    def save_run(self, path, filename):
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        full_filename = f"{filename}_{timestamp}.json"
        full_path = os.path.join(path, full_filename)

        try:
            with open(full_path, 'w') as output_file:
                json.dump(self.logs, output_file, indent=4)
            print(f"Run successfully saved to: {full_path}")
        except Exception as e:
            print(f"Failed to save run: {e}")
